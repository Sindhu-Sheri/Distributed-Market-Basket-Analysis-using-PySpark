{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project_1(Sindhu_Sheri).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdxJSAW467P-",
        "colab_type": "text"
      },
      "source": [
        "## **Project - 1(Sindhu_Sheri)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCFGUPwJnjUj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0435f8e1-f928-44f3-dd9f-6b779d3948ed"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bYilnmMwMSR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "039f2e19-bf3c-4025-aa8b-f708c8406ab4"
      },
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/26/198fc8c0b98580f617cb03cb298c6056587b8f0447e20fa40c5b634ced77/pyspark-3.0.1.tar.gz (204.2MB)\n",
            "\u001b[K     |████████████████████████████████| 204.2MB 69kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 45.2MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612243 sha256=f7bba69ebad35a67c2945318c30b389d9fc6d4477eb5b02eff8c5ed505fb7e20\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/bd/07/031766ca628adec8435bb40f0bd83bb676ce65ff4007f8e73f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18ZrM0gswjXV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "from pyspark import SparkContext, SparkConf\n",
        "import itertools\n",
        "from collections import Counter\n",
        "from itertools import combinations"
      ],
      "execution_count": 203,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcAJZDFXwlol",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creating Spark context with necessary configuration\n",
        "sparkcontext = SparkContext(\"local\",\"PySpark Aprori example\")"
      ],
      "execution_count": 213,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5PrVJYvwwOI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file = \"drive/My Drive/bigdata_assignments/browsing.txt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGHdSet21ky3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MIN_SUPPORT = 85\n",
        "MIN_CONFIDENCE = 0.9"
      ],
      "execution_count": 200,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_4bhrJww6vK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fileRDD = sparkcontext.textFile(file)\n",
        "itemset = fileRDD.map(lambda line: sorted([str(item) for item in line.strip().split(' ')]))"
      ],
      "execution_count": 215,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-e3M-4qxqLi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Broadcast the itemset to all executors. Covert to frozenset as it can be hashed and can be used to lookup in O(1).\n",
        "broadcasted_itemset = sparkcontext.broadcast(itemset.map(lambda x: frozenset(x)).collect())"
      ],
      "execution_count": 217,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTNLc0kEsCRx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generates next candidate given previous frequent item set and k\n",
        "def generate_next_candidate(prev_frequent_k, k):\n",
        "    next_candidate = []\n",
        "    # Create next candidate set by joinining previous fequent set with itself\n",
        "    for index, left in enumerate(prev_frequent_k) :\n",
        "      for right in prev_frequent_k[index + 1:] :\n",
        "        # If first k-2 elements are same, join\n",
        "        if len(left.intersection(right)) == k - 2 :\n",
        "          next_candidate.append(left | right)\n",
        "    return next_candidate"
      ],
      "execution_count": 218,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iW7JDMZa_-ga",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generates frequent item set\n",
        "# sparkContext - spark context\n",
        "# candidate_k - candidate set\n",
        "# broadcasted_itemset - broadcasted itemset\n",
        "# supprot - minimum support required\n",
        "def generate_frequent_k(sparkContext, candidate_k, brodcasted_item_transaction_index_map, support):\n",
        "    def get_frequent(candidate):\n",
        "        current_transactions = None\n",
        "        for item in candidate :\n",
        "          if current_transactions == None:\n",
        "            # If current_transactions is None (i.e this is the first loop), initialise current_transactions to set of transation indexes for this item.\n",
        "            current_transactions = brodcasted_item_transaction_index_map.value[item]\n",
        "          else:\n",
        "            # Take intersection between current_transactions and indexes of transactions where item is present.\n",
        "            # If I1 is present in T1, T2, T5 and I2 is present in T2, T4, T6. \n",
        "            # To efficiently find out all transactions where (I1, I2) both are present we can take the intersection of both the transaction_indexes. This gives us (T2) for the example. \n",
        "            current_transactions = current_transactions.intersection(brodcasted_item_transaction_index_map.value[item])\n",
        "        # Once we have all the indexes in which the candidate is present, support can be calculated using length of the transaction indexes.\n",
        "        candidate_support = len(current_transactions)\n",
        "        if candidate_support >= support:\n",
        "            # Return only if candidate_support is greater than the minimum support required.\n",
        "            return frozenset(candidate), candidate_support\n",
        "\n",
        "    # From the candidate set, take only the frequent itemsets\n",
        "    frequent_k = sparkContext.parallelize(candidate_k).map(get_frequent).filter(lambda x: x).collect()\n",
        "    return frequent_k"
      ],
      "execution_count": 219,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkBihuvhGAb7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "item_transaction_index_map = {}\n",
        "\n",
        "# Returns a list of tuples where each tuple is of form (item, transaction_set). transaction_set is set of indexes of transactions where the item is present.\n",
        "def get_item_transaction_index(transaction_with_index):\n",
        "  transaction, transaction_index = transaction_with_index\n",
        "  curr_item_transaction_index_map = {}\n",
        "  for item in transaction:\n",
        "    if item in curr_item_transaction_index_map :\n",
        "      curr_item_transaction_index_map[item].add(transaction_index)\n",
        "    else :\n",
        "      curr_item_transaction_index_map[item] = set([transaction_index])\n",
        "  return [(item, transactions_set) for item, transactions_set in curr_item_transaction_index_map.items()]\n",
        "\n",
        "# brodcasted_item_transaction_index_map is a dictionary mapping each item to set of transation indexes it is present in. \n",
        "brodcasted_item_transaction_index_map = sparkcontext.broadcast(dict(itemset.zipWithIndex() # zipWithIndex gives (itemset, index). itemset is transaction.\n",
        "                                                                            # Call get_item_transaction_index to get item_transaction_index map for each transation in form of list of (item, transaction_set).\n",
        "                                                                            # flatMap converts to convert list of lists to a single list\n",
        "                                                                           .flatMap(get_item_transaction_index) \n",
        "                                                                            # We have list of (item, transaction_set) from previous step. reduce using key (item) to get final list of (item, transaction_set) considering all transations\n",
        "                                                                           .reduceByKey(lambda transationIndex1, transationIndex2 : \n",
        "                                                                                         transationIndex1 | transationIndex2)\n",
        "                                                                           .collect()))"
      ],
      "execution_count": 221,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYVjs9VFubnq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "frequent_set_support = dict()\n",
        "\n",
        "k = 1\n",
        "candidate_k = itemset.flatMap(lambda x: set(x)).distinct().collect()\n",
        "candidate_k = [{x} for x in candidate_k]\n",
        "\n",
        "while k <= 4:\n",
        "  frequent_k = generate_frequent_k(sparkcontext, candidate_k, brodcasted_item_transaction_index_map, MIN_SUPPORT)\n",
        " \n",
        "  # Store frequent_set to support mapping in frequent_set_support. This helps us in calculating confidence in O(1).\n",
        "  for frequent_set, support in frequent_k:\n",
        "    frequent_set_support[frequent_set] = support\n",
        "\n",
        "  k += 1\n",
        "  if k > 4:\n",
        "    break\n",
        "\n",
        "  candidate_k = generate_next_candidate([set(item) for item in map(lambda x: x[0], frequent_k)], k)"
      ],
      "execution_count": 223,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOy3fyxo6Wik",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Returns confidence of association rule\n",
        "def get_confidence(left, right, broadcasted_frequent_set_support):\n",
        "  frequent_set_support_dict = broadcasted_frequent_set_support.value\n",
        "  print(type(frequent_set_support_dict))\n",
        "  return frequent_set_support_dict[frozenset(left | right)]/frequent_set_support_dict[frozenset(left)]"
      ],
      "execution_count": 224,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCWDJkpV6fJ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Given frequent_set and right hand side item, returns left hand set and right hand set\n",
        "# If frequent_set is I1, I2, T3 and right_item is T2 returns set(I1, I3), set(T2) - I1, I3 → T2\n",
        "def get_rule(frequent_set, right_item):\n",
        "  left = set(frequent_set)\n",
        "  left.remove(right_item)\n",
        "  return left, set([right_item])"
      ],
      "execution_count": 225,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmHzZPAV6h3X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Returns list of tuple of associtaion rule and confidence whose confidence is greater than MIN_CONFIDENCE\n",
        "def get_associate_rule_confidence(frequent_item_support, broadcasted_frequent_set_support):\n",
        "  frequent_set, support = frequent_item_support\n",
        "  rules = []\n",
        "  for right_item in frequent_set:\n",
        "    left, right = get_rule(frequent_set, right_item)\n",
        "    confidence = get_confidence(left, right, broadcasted_frequent_set_support)\n",
        "    if confidence >= MIN_CONFIDENCE:\n",
        "      # Pretty print association rule and confidence\n",
        "      rules.append((\",\".join(left) + \" → \" + right_item, confidence))\n",
        "  return rules"
      ],
      "execution_count": 226,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RICg5hc6lS-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "broadcasted_frequent_set_support = sparkcontext.broadcast(frequent_set_support)"
      ],
      "execution_count": 228,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bb6BbGap6pkV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Delete folder if exists\n",
        "!rm -rf /content/associationRulesWithSupport\n",
        "\n",
        "(sparkcontext.parallelize(frequent_set_support.items()) # Parallelize frequent_set_support items (frequent_set, support)\n",
        "            .filter(lambda x : len(x[0]) > 1) # Only consider frequent sets of size > 1 as we need atleast 2 items for generating association rules\n",
        "            .flatMap(lambda x : get_associate_rule_confidence(x, broadcasted_frequent_set_support)) # Flat map get_associate_rule_confidence\n",
        "            .sortBy(lambda x : x[1], False) # Sort by confidence in descending order\n",
        "            .map(lambda x : x[0] + \"; Confidence=\" + str(round(x[1]*100, 2)) + \"%\")  # Generate pretty string for saving\n",
        "            .coalesce(1)  # Coalesce to one partition to get a single file output\n",
        "            .saveAsTextFile(\"/content/associationRulesWithSupport\"))"
      ],
      "execution_count": 231,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgOu60i46yZr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7eba0aeb-cfc5-4dbc-d8e5-59e8b83a3ae4"
      },
      "source": [
        "!cat /content/associationRulesWithSupport/part-00000"
      ],
      "execution_count": 232,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DAI93865 → FRO40251; Confidence=100.0%\n",
            "GRO85051,ELE17451 → FRO40251; Confidence=100.0%\n",
            "GRO85051,ELE26917 → FRO40251; Confidence=100.0%\n",
            "GRO85051,GRO73461 → FRO40251; Confidence=100.0%\n",
            "GRO85051,GRO94758 → FRO40251; Confidence=100.0%\n",
            "GRO85051,DAI55911 → FRO40251; Confidence=100.0%\n",
            "SNA18336,DAI23334 → DAI62779; Confidence=100.0%\n",
            "DAI88079,DAI62779 → FRO40251; Confidence=100.0%\n",
            "ELE92920,DAI23334 → DAI62779; Confidence=100.0%\n",
            "SNA55762,GRO85051 → FRO40251; Confidence=100.0%\n",
            "SNA45677,GRO85051 → FRO40251; Confidence=100.0%\n",
            "GRO85051,GRO21487 → FRO40251; Confidence=100.0%\n",
            "GRO85051,SNA80324 → FRO40251; Confidence=100.0%\n",
            "GRO85051,DAI75645 → FRO40251; Confidence=100.0%\n",
            "GRO38814,GRO85051 → FRO40251; Confidence=100.0%\n",
            "GRO85051,DAI83948 → FRO40251; Confidence=100.0%\n",
            "GRO85051,FRO53271 → FRO40251; Confidence=100.0%\n",
            "GRO85051,ELE74009 → FRO40251; Confidence=100.0%\n",
            "DAI31081,GRO85051 → FRO40251; Confidence=100.0%\n",
            "GRO85051,ELE20847 → FRO40251; Confidence=100.0%\n",
            "GRO85051,DAI85309 → FRO40251; Confidence=100.0%\n",
            "FRO92469,ELE20847 → FRO40251; Confidence=100.0%\n",
            "SNA18336,ELE92920,ELE17451 → DAI62779; Confidence=100.0%\n",
            "GRO85051,DAI62779,ELE17451 → FRO40251; Confidence=100.0%\n",
            "GRO85051,SNA80324,ELE17451 → FRO40251; Confidence=100.0%\n",
            "GRO85051,DAI75645,ELE17451 → FRO40251; Confidence=100.0%\n",
            "SNA18336,ELE92920,GRO15017 → DAI62779; Confidence=100.0%\n",
            "SNA18336,DAI62779,GRO81087 → ELE92920; Confidence=100.0%\n",
            "SNA18336,ELE92920,GRO81087 → DAI62779; Confidence=100.0%\n",
            "GRO85051,SNA80324,DAI62779 → FRO40251; Confidence=100.0%\n",
            "GRO85051,DAI75645,DAI62779 → FRO40251; Confidence=100.0%\n",
            "GRO85051,DAI75645,SNA80324 → FRO40251; Confidence=100.0%\n",
            "GRO85051 → FRO40251; Confidence=99.92%\n",
            "GRO85051,DAI62779 → FRO40251; Confidence=99.74%\n",
            "DAI88079,DAI75645 → FRO40251; Confidence=99.33%\n",
            "DAI88079,GRO73461 → FRO40251; Confidence=99.31%\n",
            "DAI88079,ELE17451 → FRO40251; Confidence=99.19%\n",
            "GRO38636 → FRO40251; Confidence=99.07%\n",
            "FRO92469,GRO73461 → FRO40251; Confidence=99.06%\n",
            "ELE12951 → FRO40251; Confidence=99.06%\n",
            "SNA95666,DAI63921 → FRO85978; Confidence=98.97%\n",
            "FRO92469,DAI91290 → FRO40251; Confidence=98.94%\n",
            "FRO92469,DAI62779,ELE17451 → FRO40251; Confidence=98.94%\n",
            "GRO85051,DAI42493 → FRO40251; Confidence=98.91%\n",
            "DAI88079 → FRO40251; Confidence=98.67%\n",
            "ELE92920,DAI85309,ELE17451 → DAI62779; Confidence=98.54%\n",
            "FRO92469 → FRO40251; Confidence=98.35%\n",
            "FRO92469,DAI62779 → FRO40251; Confidence=98.35%\n",
            "FRO92469,ELE17451 → FRO40251; Confidence=98.18%\n",
            "FRO92469,DAI75645 → FRO40251; Confidence=98.05%\n",
            "DAI88079,SNA80324 → FRO40251; Confidence=97.89%\n",
            "FRO92469,DAI75645,SNA80324 → FRO40251; Confidence=97.48%\n",
            "FRO92469,SNA80324 → FRO40251; Confidence=97.44%\n",
            "DAI43868 → SNA82528; Confidence=97.3%\n",
            "FRO92469,DAI55148 → FRO40251; Confidence=97.22%\n",
            "DAI23334,ELE17451 → DAI62779; Confidence=97.0%\n",
            "SNA18336,ELE92920,DAI42493 → DAI62779; Confidence=96.84%\n",
            "FRO85978,SNA93860,ELE59028 → DAI62779; Confidence=96.72%\n",
            "FRO92469,SNA80324,DAI62779 → FRO40251; Confidence=96.52%\n",
            "SNA18336,ELE92920,DAI85309 → DAI62779; Confidence=96.4%\n",
            "SNA18336,DAI85309,ELE17451 → DAI62779; Confidence=95.8%\n",
            "ELE92920,GRO81087 → DAI62779; Confidence=95.71%\n",
            "ELE92920,SNA90094 → DAI62779; Confidence=95.65%\n",
            "SNA53220,SNA93860,FRO19221 → DAI62779; Confidence=95.61%\n",
            "DAI23334 → DAI62779; Confidence=95.45%\n",
            "ELE92920,DAI85309 → DAI62779; Confidence=95.02%\n",
            "SNA18336,ELE92920 → DAI62779; Confidence=94.95%\n",
            "SNA18336,FRO40251,ELE92920 → DAI62779; Confidence=94.68%\n",
            "SNA18336,ELE92920,ELE32164 → DAI62779; Confidence=93.75%\n",
            "SNA18336,DAI62779,ELE17451 → ELE92920; Confidence=93.44%\n",
            "ELE92920,ELE26917 → DAI62779; Confidence=93.4%\n",
            "SNA18336,GRO81087 → DAI62779; Confidence=93.14%\n",
            "SNA18336,GRO81087 → ELE92920; Confidence=93.14%\n",
            "GRO85051,SNA80324,ELE17451 → DAI62779; Confidence=93.04%\n",
            "SNA18336,GRO15017 → DAI62779; Confidence=92.92%\n",
            "ELE92920,DAI83733 → DAI62779; Confidence=92.79%\n",
            "SNA18336,GRO15017,DAI62779 → ELE92920; Confidence=92.38%\n",
            "SNA18336,ELE17451 → DAI62779; Confidence=91.39%\n",
            "SNA18336,ELE32164,DAI62779 → ELE92920; Confidence=90.0%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}